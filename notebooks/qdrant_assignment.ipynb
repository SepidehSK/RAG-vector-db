{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semestral Home Assignment \n",
        "In the semestral home assignment you are tasked with designing and implementing a production ready information retrieval (IR) system with the use of Qdrant. <br>\n",
        "First will need to implement scalable Qdrant cluster with the principles of NoSQL (sharding, replication quorum). <br>\n",
        "Then, you will implement the vector search with Qdrant using all the advanced features of the vector database. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Any, cast, Callable\n",
        "\n",
        "from datasets import load_dataset\n",
        "from datasets.dataset_dict import DatasetDict\n",
        "from datasets.dataset_dict import Dataset\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import models\n",
        "from qdrant_client.http.models.models import QueryResponse\n",
        "from fastembed import TextEmbedding, SparseTextEmbedding, LateInteractionTextEmbedding\n",
        "from fastembed.sparse.sparse_embedding_base import SparseEmbedding\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from notebooks.utils import evaluate_retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load environment variables. **Do not forget to create a .env file in the root directory based on the .env.example file**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(\"./.env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start up local instance of Qdrant through docker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!docker run -p 6335:6333 -p 6336:6334 -d --name qdrant-server qdrant/qdrant:v1.16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate the Qdrant client by connecting to the server running as a docker container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(host=os.environ[\"QDRANT_HOST\"], port=int(os.environ[\"QDRANT_PORT\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1 - Data Loading\n",
        "Load the data from the Hugging Face dataset [Zovi3/pa195_semestral_assignment](https://huggingface.co/datasets/Zovi3/pa195_semestral_assignment/upload/main), explore it and extract/preprocess it if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import query dataset from https://huggingface.co/datasets/Zovi3/pa195_semestral_assignment/tree/main\n",
        "query_dataset: Dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import documents dataset from https://huggingface.co/datasets/Zovi3/pa195_semestral_assignment/tree/main\n",
        "documents: Dataset = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Within the homework you will work with `sentence-transformers/all-MiniLM-L6-v` from fastembed library. <br>\n",
        "These embedding are precomputed for you in the assignment dataset, but you will need to used model when running the queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Embeddings are precomputed so you can save some memory by not loading the model\n",
        "# embedding_model = TextEmbedding('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embedding_model_size = 384"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sparse Retrieval Model\n",
        "Some queries require the prioritization of the certain keywords. <br>\n",
        "Therefor, you will need to use BM25 algorithm to boost the documents with these keywords during retrieval. <br>\n",
        "Note that BM25 is not taken into account in the dataset, so you will need to apply when uploading and indexing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_model = SparseTextEmbedding(\"Qdrant/bm25\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Vector Model\n",
        "It is general good practice to include reranking model in the IR system. <br>\n",
        "Reranking uses stronger model to select the most relevant documents from the initial retrieval. <br>\n",
        "You will implement reranking with multi-vector late interaction embedding ColBERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Embeddings are precomputed so you can save some memory by not loading the model\n",
        "# multi_vector_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
        "multi_vector_model_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Database Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2 - Data Modelling\n",
        "In this task you will create proper data model for your data including vector representations, index configuration, distance functions and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 2.1 - HNSW Index Configuration\n",
        "Configure the HNSW index for the retrieval. <br>\n",
        "**Change the ef_construct parameter to 64 to speed the build time at the cost of the recall.** <br>\n",
        "We do this for practical reasons, to enable you iterate over the notebook faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change ef_construct parameter to 64 to speed the build time at the cost of the recall\n",
        "ef_construct = 64\n",
        "# TODO Configure HNSW index\n",
        "hnsw_config=None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 2.2 - Collection Creation\n",
        "Create model for your data. You should create three vector representations for your data. <br>\n",
        "There should be one representation for each model defined above. <br>\n",
        "For multi-vector model make sure to disable the vector index since it will be used only for reranking. <br>\n",
        "Also, do not forget that multi-vector computation of similarity is not done only through the cosine similarity (check the lecture for more info). <br>\n",
        "Configure proper modifier for the sparse vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLLECTION_NAME = \"ms_macro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    client.delete_collection(COLLECTION_NAME)\n",
        "    print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
        "except: \n",
        "    print(f\"Collection {COLLECTION_NAME} does not exist\")\n",
        "\n",
        "\n",
        "# TODO: Configure collection creation  \n",
        "collection_created = False #client.create_collection(\n",
        "#    collection_name=COLLECTION_NAME,\n",
        "# )\n",
        "\n",
        "if collection_created:\n",
        "    print(f\"Created collection '{COLLECTION_NAME}'.\")\n",
        "else:\n",
        "    print(\"Collection creation failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 2.3 - Create Payload Index & Disable Quantization\n",
        "Configure keyword payload index for the `groups` field. Make sure that payload index is on-disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create payload index\n",
        "payload_index_created = False # client.create_payload_index(\n",
        "#    collection_name=COLLECTION_NAME,\n",
        "# )\n",
        "\n",
        "if payload_index_created:\n",
        "    print(f\"Payload index created for field 'groups'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3 - Data Upload\n",
        "Upload vector embeddings and metadata to the created collection, make sure to upload the vectors metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points: list[models.PointStruct] = []\n",
        "\n",
        "doc: dict[str, Any]\n",
        "for doc in documents: # type: ignore\n",
        "    # TODO: Implement data upload\n",
        "    pass\n",
        "\n",
        "print(\"Upserting documents...\")\n",
        "client.upload_points(collection_name=COLLECTION_NAME, points=points, batch_size=128)\n",
        "\n",
        "print(f\"Collection info: {client.get_collection(COLLECTION_NAME).points_count} points in collection\")\n",
        "assert client.get_collection(COLLECTION_NAME).points_count == len(documents), f\"Expected {len(documents)} points in collection, got {client.get_collection(COLLECTION_NAME).points_count}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4 - Design Complex Query\n",
        "Your task is to design a complex query that will include hybrid search, filtering, reranking and metadata boosting. <br>\n",
        "**The result of this task should be one Qdrant query (do not add any postprocessing logic outside of the Qdrant query)!**\n",
        " \n",
        "**Subtasks:**\n",
        "1. Define query filter with relation to the `groups` field, do not forget there can be filter values in the query.\n",
        "    - Think about in which prefetch you should apply the filter.\n",
        "2. Define sparse and dense search prefetche, the limit for the retrieval should be 100 objects.\n",
        "3. Define fusion of the two rankings with Reciprocal Rank Fusion (RRF).\n",
        "4. Rerank the results with ColBERT multi-vector model, use 50 documents for reranking.\n",
        "5. Boost the results with metadata weighting, use `group_1` with weight 0.05 and `group_2` with weight 0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_context_retrieval(query: dict[str, Any]) -> QueryResponse:\n",
        "    # TODO: Implement correct embeddings usage\n",
        "    query_dense_embedding: list[float] = []\n",
        "    query_sparse_embedding: SparseEmbedding = None\n",
        "    query_multi_vector_embedding: list[list[float]] = []\n",
        "\n",
        "    # Task 4.1 - Define query filter\n",
        "    filter_condition : models.Filter = None  # TODO: Implement filters\n",
        "\n",
        "    \n",
        "    sparse_limit = 100\n",
        "    dense_limit = 100\n",
        "    # Task 4.2 - Define sparse and dense search. Set their limit to 100.\n",
        "    prefetch_sparse_and_dense_search: list[models.Prefetch] = [\n",
        "        # TODO: Implement sparse and dense prefetches\n",
        "    ]\n",
        "\n",
        "    # Task 4.3 - Define fusion of the two rankings (set the k parameter of the query to 60 to mitigate effect of high rankings)\n",
        "    rff_k = 60\n",
        "    prefetch_fused_rankings: list[models.Prefetch] = [\n",
        "        # TODO: Implement rank fusion\n",
        "    ]\n",
        "\n",
        "    # Task 4.4 - Rerank the results with ColBERT multi-vector model taking 50 documents.\n",
        "    reranking_limit = 50\n",
        "    prefetch_multi_vector_reranking: list[models.Prefetch] = [\n",
        "        # TODO: Implement multi-vector reranking\n",
        "    ]\n",
        "    \n",
        "    group_1_boost_weight = 0.05\n",
        "    group_2_boost_weight = 0.1\n",
        "    final_query_limit = 10\n",
        "    # Task 4.5 - Boost following \"groups\" in the search: \"group_1\" with weight 0.05 and \"group_2\" with weight 0.1\n",
        "    final_result: QueryResponse = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        # TODO: Implement final query with metadata boosting\n",
        "        # TODO: This query should be built from all the prefetches\n",
        "    )\n",
        "\n",
        "    return final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_retrieval_precision = evaluate_retrieval(rag_context_retrieval, query_dataset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
